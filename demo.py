import gradio as gr
import os
import boto3
from openai import OpenAI
from botocore.config import Config
import json
from translate import GuideBased
import re
from dotenv import load_dotenv
from ape import APE
ape = APE()
rewrite = GuideBased()

load_dotenv()

# Initialize the LLM client
bedrock_runtime = boto3.client(service_name='bedrock-runtime', region_name=os.getenv('REGION_NAME'))
client = OpenAI()

default_system = "you have profound knowledge and hands on experience in field of software engineering and artificial intelligence, you are also an experienced solution architect in Amazon Web Service and have expertise to impelment model application development with AWS in consdieration of well architect and industry best practice."
bedrock_default_system = default_system
openai_default_system = default_system

def generate_prompt(original_prompt, level):
    if level == '一次生成':
        result = rewrite(original_prompt) #, cost
        return [gr.Textbox(label="我们为您生成的prompt",
                   value=result,
                   lines=3,show_copy_button=True,interactive=False)]+[gr.Textbox(visible=False)]*2
        
    elif level == '多次生成':
        candidates = []
        for i in range(3):
            result = rewrite(original_prompt)
            candidates.append(result)
        judge_result = rewrite.judge(candidates)
        textboxes = []
        for i in range(3):
            is_best = 'Y' if judge_result == i else 'N'
            textboxes.append(
                gr.Textbox(label=f"我们为您生成的prompt #{i+1} {is_best}",
                   value=candidates[i],
                   lines=3,show_copy_button=True,visible=True,interactive=False)
            )
        return textboxes

def ape_prompt(original_prompt, user_data):
    result = ape(initial_prompt, 1, json.loads(user_data))
    return [gr.Textbox(label="我们为您生成的prompt",
                       value=result['prompt'],
                       lines=3,show_copy_button=True,interactive=False)]+[gr.Textbox(visible=False)]*2

def generate_bedrock_response(prompt, model_id):
    """
    This function generates a test dataset by invoking a model with a given prompt.

    Parameters:
    prompt (str): The user input prompt.

    Returns:
    matches (list): A list of questions generated by the model, each wrapped in <case></case> XML tags.
    """
    message = {
        "role": "user",
        "content": [
            # {"type": "image", "source": {"type": "base64", "media_type": "image/jpeg", "data": content_image}},
            {"type": "text", "text": prompt}
        ]
    }
    messages = [message]
    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 4000,
        "messages": messages,
        "system": bedrock_default_system,
    })
    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)
    response_body = json.loads(response.get('body').read())
    return(response_body['content'][0]['text'])

def generate_openai_response(prompt, model_id):
    completion = client.chat.completions.create(
        model = model_id,
        messages=[
            {"role": "system", "content": openai_default_system},
            {"role": "user", "content": prompt}
        ]
    )
    return(completion.choices[0].message.content)
 
def evaluate_prompt(revised_prompt, openai_model_id, aws_model_id):
    openai_result = generate_openai_response(revised_prompt, openai_model_id)
    aws_result = generate_bedrock_response(revised_prompt, aws_model_id)
    return openai_result, aws_result

def insert_kv(user_prompt, kv_string):
    # Split the key-value string by ';' to get individual pairs
    kv_pairs = kv_string.split(';')
    for pair in kv_pairs:
        if ':' in pair:
            key, value = pair.split(':', 1)  # Only split on the first ':'
            user_prompt = user_prompt.replace(f"{{{key}}}", value).replace(f"<{key}>", value)
    return user_prompt

with gr.Blocks(title='Automatic Prompt Engineering', theme='soft', css="#textbox_id textarea {color: red}") as demo:
    with gr.Tab("Prompt 生成"):
        gr.Markdown('# Automatic Prompt Engineering')
        original_prompt = gr.Textbox(label="请输入您的原始prompt",
                                        lines=3)
        gr.Markdown('其中用户自定义变量使用{\{xxx\}}表示，例如{\{document\}}')
        with gr.Row():
            with gr.Column(scale=2):
                level = gr.Radio(['一次生成', '多次生成'], label="优化等级", value='一次生成')
                b1 = gr.Button("优化prompt")
            with gr.Column(scale=2):
                user_data = gr.Textbox(label="测试数据JSON",
                                        lines=2)
                b2 = gr.Button("APE优化prompt")
        textboxes = []
        for i in range(3):
            t = gr.Textbox(label="我们为您生成的prompt", elem_id="textbox_id", lines=3,show_copy_button=True,interactive=False,visible=False if i>0 else True)
            textboxes.append(t)
        log = gr.Markdown('')
        b1.click(generate_prompt, inputs=[original_prompt, level], outputs=textboxes)
        b2.click(ape_prompt, inputs=[original_prompt, user_data], outputs=textboxes)
    with gr.Tab("Prompt 评估"):
        user_prompt = gr.Textbox(label="请输入您要评估的prompt", lines=3)
        kv_input = gr.Textbox(label="[可选]输入需要替换的模版参数", placeholder="参考格式: key1:value1;key2:value2", lines=2)

        insert_button = gr.Button("替换模版参数")
        insert_button.click(insert_kv, inputs=[user_prompt, kv_input], outputs=user_prompt)
        with gr.Row():
            # https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo
            openai_model_dropdown = gr.Dropdown(label="选择 OpenAI 模型", choices=['gpt-3.5-turbo', 'gpt-4-32k', 'gpt-4-1106-preview', 'gpt-4-turbo-preview'], value='gpt-3.5-turbo')
            # aws bedrock list-foundation-models --region us-east-1 --output json | jq -r '.modelSummaries[].modelId'
            aws_model_dropdown = gr.Dropdown(label="选择 AWS 模型",
                                choices=['anthropic.claude-instant-v1:2:100k', 'anthropic.claude-instant-v1', 'anthropic.claude-v2:0:18k', 'anthropic.claude-v2:0:100k', 'anthropic.claude-v2:1:18k', 'anthropic.claude-v2:1:200k', 'anthropic.claude-v2:1', 'anthropic.claude-v2', 'anthropic.claude-3-sonnet-20240229-v1:0', 'anthropic.claude-3-haiku-20240307-v1:0'],
                                value='anthropic.claude-3-sonnet-20240229-v1:0')
        evaluate_button = gr.Button("评估prompt")
        with gr.Row():
            openai_output = gr.Textbox(label="OpenAI 输出", lines=3, interactive=False)
            aws_output = gr.Textbox(label="AWS Bedrock 输出", lines=3, interactive=False)
        evaluate_button.click(evaluate_prompt, inputs=[user_prompt, openai_model_dropdown, aws_model_dropdown], outputs=[openai_output, aws_output])
demo.launch()